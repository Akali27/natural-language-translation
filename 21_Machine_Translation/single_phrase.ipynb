{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "class TokenizerWrap(Tokenizer):\n",
    "    def __init__(self, texts, padding, reverse=False, num_words = None):\n",
    "\n",
    "        Tokenizer.__init__(self, num_words = num_words)\n",
    "        self.fit_on_texts(texts)\n",
    "        self.index_to_word = dict(zip(self.word_index.values(),\n",
    "                                      self.word_index.keys()))\n",
    "        self.tokens = self.texts_to_sequences(texts)\n",
    "        if reverse:\n",
    "            self.tokens = [list(reversed(x)) for x in self.tokens]\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            truncating = 'post'\n",
    "        self.num_tokens = [len(x) for x in self.tokens]\n",
    "        self.max_tokens = np.mean(self.num_tokens) \\\n",
    "                          + 2 * np.std(self.num_tokens)\n",
    "        self.max_tokens = int(self.max_tokens)\n",
    "        self.tokens_padded = pad_sequences(self.tokens,\n",
    "                                           maxlen = self.max_tokens,\n",
    "                                           padding = padding,\n",
    "                                           truncating = truncating)\n",
    "\n",
    "    def token_to_word(self, token):\n",
    "        word = \" \" if token == 0 else self.index_to_word[token]\n",
    "        return word\n",
    "        \n",
    "    def tokens_to_string(self, tokens):\n",
    "        words = [self.index_to_word[token]\n",
    "                 for token in tokens\n",
    "                 if token != 0]\n",
    "        text = \" \".join(words)\n",
    "        return text\n",
    "\n",
    "    def text_to_tokens(self, text, reverse = False, padding = False):\n",
    "        tokens = self.texts_to_sequences([text])\n",
    "        tokens = np.array(tokens)\n",
    "\n",
    "        if reverse:\n",
    "            tokens = np.flip(tokens, axis = 1)\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            truncating = 'post'\n",
    "        if padding:\n",
    "            tokens = pad_sequences(tokens,\n",
    "                                   maxlen = self.max_tokens,\n",
    "                                   padding = 'pre',\n",
    "                                   truncating = truncating)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "def load_inputs():\n",
    "    \n",
    "    tokenizer_src = joblib.load('tokenizer_src.pkl')\n",
    "    tokenizer_dest = joblib.load('tokenizer_dest.pkl')\n",
    "    \n",
    "    return (tokenizer_src, tokenizer_dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_src, tokenizer_dest = load_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    from tensorflow.python.keras.models import Model\n",
    "    from tensorflow.python.keras.layers import Input, Dense, GRU, Embedding\n",
    "    \n",
    "    num_words = 10000\n",
    "    encoder_input = Input(shape = (None, ), name ='encoder_input')\n",
    "    embedding_size = 128\n",
    "    encoder_embedding = Embedding(input_dim = num_words, output_dim = embedding_size,\n",
    "                              name = 'encoder_embedding')\n",
    "    state_size = 512\n",
    "    encoder_gru1 = GRU(state_size, name = 'encoder_gru1', return_sequences = True)\n",
    "    encoder_gru2 = GRU(state_size, name = 'encoder_gru2', return_sequences = True)\n",
    "    encoder_gru3 = GRU(state_size, name = 'encoder_gru3', return_sequences = False)\n",
    "\n",
    "    def connect_encoder():\n",
    "        net = encoder_input\n",
    "        net = encoder_embedding(net)\n",
    "        net = encoder_gru1(net)\n",
    "        net = encoder_gru2(net)\n",
    "        net = encoder_gru3(net)\n",
    "        encoder_output = net\n",
    "        return encoder_output\n",
    "    \n",
    "    encoder_output = connect_encoder()\n",
    "    \n",
    "    decoder_initial_state = Input(shape = (state_size,), name  = 'decoder_initial_state')\n",
    "    decoder_input = Input(shape=(None, ), name = 'decoder_input')\n",
    "    decoder_embedding = Embedding(input_dim = num_words, output_dim = embedding_size,\n",
    "                                  name = 'decoder_embedding')\n",
    "    decoder_gru1 = GRU(state_size, name = 'decoder_gru1', return_sequences = True)\n",
    "    decoder_gru2 = GRU(state_size, name = 'decoder_gru2', return_sequences = True)\n",
    "    decoder_gru3 = GRU(state_size, name = 'decoder_gru3', return_sequences = True)\n",
    "    decoder_dense = Dense(num_words, activation = 'linear', name = 'decoder_output')\n",
    "    \n",
    "    def connect_decoder(initial_state):\n",
    "        net = decoder_input\n",
    "        net = decoder_embedding(net)\n",
    "        net = decoder_gru1(net, initial_state = initial_state)\n",
    "        net = decoder_gru2(net, initial_state = initial_state)\n",
    "        net = decoder_gru3(net, initial_state = initial_state)\n",
    "        decoder_output = decoder_dense(net)\n",
    "\n",
    "        return decoder_output\n",
    "    \n",
    "    decoder_output = connect_decoder(initial_state = encoder_output)\n",
    "    \n",
    "    model = Model(inputs = [encoder_input, decoder_input],\n",
    "                    outputs = [decoder_output])\n",
    "    model.load_weights('21_checkpoint.keras')\n",
    "    \n",
    "    model_encoder = Model(inputs = [encoder_input],\n",
    "                      outputs = [encoder_output])\n",
    "    decoder_output = connect_decoder(initial_state = decoder_initial_state)\n",
    "\n",
    "    model_decoder = Model(inputs = [decoder_input, decoder_initial_state],\n",
    "                      outputs = [decoder_output])\n",
    "    \n",
    "    return (model, model_encoder, model_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sonik/anaconda3/envs/pythondata/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "model, model_encoder, model_decoder = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_text, tokenizer_src = tokenizer_src, tokenizer_dest = tokenizer_dest):\n",
    "    token_start = 3\n",
    "    token_end = 4\n",
    "    input_tokens = tokenizer_src.text_to_tokens(text = input_text,reverse = True,padding = True)\n",
    "    initial_state = model_encoder.predict(input_tokens)\n",
    "    max_tokens = tokenizer_dest.max_tokens\n",
    "    shape = (1, max_tokens)\n",
    "    decoder_input_data = np.zeros(shape = shape, dtype = np.int)\n",
    "    token_int = token_start\n",
    "    output_text = ''\n",
    "    count_tokens = 0\n",
    "    while token_int != token_end and count_tokens < max_tokens:\n",
    "        decoder_input_data[0, count_tokens] = token_int\n",
    "        x_data = \\\n",
    "        {\n",
    "            'decoder_initial_state': initial_state,\n",
    "            'decoder_input': decoder_input_data\n",
    "        }\n",
    "        decoder_output = model_decoder.predict(x_data)\n",
    "        token_onehot = decoder_output[0, count_tokens, :]\n",
    "        token_int = np.argmax(token_onehot)\n",
    "        sampled_word = tokenizer_dest.token_to_word(token_int)\n",
    "        output_text += \" \" + sampled_word\n",
    "        count_tokens += 1\n",
    "    output_tokens = decoder_input_data[0]\n",
    "    return output_text[1:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aprobación de la gestión de la comisión'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('welcome to the parliament')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'el parlamento rechaza la solicitud de voto'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('hope you feel good today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'quisiera hacer una observación'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('I would like to introduce you..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¿hay alguna observación'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('what is you name?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'el deporte de la carne'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('people of the country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'el sistema de reacción rápida es un ejemplo'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('the number of lost people is increasing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'el parlamento aprueba la resolución'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('the number of countries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'me gustaría que esto'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('i am going back home')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'me refiero a la cuestión de la'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('I am a good person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'señor presidente'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('Mr. President')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'me gustaría que'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('I am leaving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'la votación tendrá lugar mañana a las 12 00 horas'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('the session is over, thank you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'la enmienda nº 1'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('our proposal is to do')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aplausos'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('how are you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'me gustaría que'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('i want to sleep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¿qué se'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('what is going on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'el sr ha sido'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('she lost a book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¿hay alguna observación'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('where is my car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'el sr ha hecho'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('she likes to write')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
