{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data files and save them on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(old_file_address, new_file_address):\n",
    "    \n",
    "    import string\n",
    "    from unicodedata import normalize\n",
    "    import re\n",
    "    \n",
    "    remove_punct_map = dict.fromkeys(map(ord, string.punctuation)) # thank you Reed!\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    \n",
    "    with open(old_file_address, 'r') as old_file, open(new_file_address, 'w') as clean_file:\n",
    "        for line in old_file:\n",
    "            \n",
    "            # normalize all Unicode characters to ASCII (maybe not nesessary)\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            \n",
    "            # remove non-printable characters\n",
    "            line = re_print.sub('', line)\n",
    "            \n",
    "            # normalize to lowercase, remove punctuation and write to a file\n",
    "            clean_file.write(line.lower().translate(remove_punct_map))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data('europarl-v7esp.txt', 'clean_spanish.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data('europarl-v7eng.txt', 'clean_english.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Observation with MRJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data contains millions of rows, so I used MRJob to handle it\n",
    "# Use boilerplate to count the number of words and lines\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import string\n",
    "\n",
    "remove_punct_map = dict.fromkeys(map(ord, string.punctuation)) #thank you Reed\n",
    "\n",
    "class TextOverview(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        line = line.translate(remove_punct_map)\n",
    "        line = line.lower()\n",
    "        \n",
    "        yield \"total_words\", len(line.split())\n",
    "        yield \"lines\", 1\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    TextOverview.run()\n",
    "    \n",
    "#python engl-span.py clean_english.txt\n",
    "#python engl-span.py clean_spanish.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count a number of unique words in each dataset:\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import string\n",
    "\n",
    "remove_punct_map = dict.fromkeys(map(ord, string.punctuation))\n",
    "\n",
    "class TextOverview(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "\n",
    "        line = line.translate(remove_punct_map)\n",
    "        line = line.lower()\n",
    "        for word in line.split():\n",
    "            yield 'words', word\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield key, len(set(values))\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    TextOverview.run()\n",
    "\n",
    "#python engl-span.py clean_english.txt\n",
    "#python engl-span.py clean_spanish.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Output for english file:***\n",
    "- \"total_words\"   48 978 039\n",
    "- \"lines\" 1 965 734\n",
    "- \"unique_words\" 133 052"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Output for spanish file:***\n",
    "- \"total_words\"   51 505 465\n",
    "- \"lines\" 1 965 734\n",
    "- \"unique_words\" 192 038"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save combined clean txt file on disk just in case\n",
    "with open('clean_english.txt', 'r') as eng, open('clean_spanish.txt', 'r') as span, open('combined.txt','w') as comb:\n",
    "    \n",
    "        for eline, sline in zip(eng, span):\n",
    "            eline = eline.rstrip()\n",
    "            sline = sline.rstrip()\n",
    "            \n",
    "            comb.write(f'{eline}\\t{sline}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pairs(doc):\n",
    "    file = open(doc, mode='r', encoding='utf-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    lines = text.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in  lines]\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge both files in one\n",
    "pairs = to_pairs('combined.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_array(eng_file_address, esp_file_address):\n",
    "    from numpy import array\n",
    "    \n",
    "    all_pairs = []\n",
    "    with open(eng_file_address, 'r') as eng, open(esp_file_address, 'r') as span:\n",
    "    \n",
    "        for eline, sline in zip(eng, span):\n",
    "            eline = eline.rstrip()\n",
    "            sline = sline.rstrip()\n",
    "\n",
    "            all_pairs.append([eline, sline])\n",
    "        \n",
    "    return array(all_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_array = create_array('clean_english.txt', 'clean_spanish.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0,5):\n",
    "    print(f'{x}. {pairs_array[x,0]} : {pairs_array[x,1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data to plk file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(pairs, filename):\n",
    "\n",
    "    from sklearn.externals import joblib\n",
    "    \n",
    "    joblib.dump(pairs, filename) \n",
    "    \n",
    "    print(f'Saved: {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(pairs_array, 'english-spanish.pkl') #64Gb wow!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = joblib.load('english-spanish.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = file[:20000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['what is the use of the most productive nuclear power station if a minor fault is enough to make not only the immediate surroundings but the whole region uninhabitable for decades',\n",
       "       'cual es el uso de la central nuclear mas productiva si un minimo fallo basta para que no solamente sus alrededores mas inmediatos sino tambien toda la region resulten inhabitables durante decadas'],\n",
       "      dtype='<U4092')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a number of unique words - 39211\n",
      "a total number of words - 1021109\n"
     ]
    }
   ],
   "source": [
    "unique = []\n",
    "words = []\n",
    "for pair in dataset:\n",
    "#     print(pair)\n",
    "    for sentence in pair:\n",
    "#         print(sentence)\n",
    "        for word in sentence.split(' '):\n",
    "#             print(word)\n",
    "            words.append(word)\n",
    "            if word not in unique:\n",
    "                unique.append(word)\n",
    "#     break  \n",
    "    \n",
    "print(f'a number of unique words - {len(unique)}')\n",
    "print(f'a total number of words - {len(words)}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = dataset[:16000], dataset[16000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset.pkl']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(dataset, 'dataset.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test.pkl']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(train, 'train.pkl')\n",
    "joblib.dump(test, 'test.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sonik/anaconda3/envs/pythondata/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
