{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data from URL and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset\n",
    "def get_data(link):\n",
    "    \n",
    "    from zipfile import ZipFile\n",
    "    import requests\n",
    "    import os\n",
    "    \n",
    "    output = 'temp.zip'\n",
    "\n",
    "    response = requests.get(link, stream=True)\n",
    "    handle = open(output, \"wb\")\n",
    "    for chunk in response.iter_content(chunk_size=512):\n",
    "        if chunk:\n",
    "            handle.write(chunk)\n",
    "    handle.close()\n",
    "\n",
    "\n",
    "    with ZipFile(output,\"r\") as zip_file:\n",
    "        for name in zip_file.namelist():\n",
    "            if name != '_about.txt':\n",
    "                filename = name\n",
    "        zip_file.extractall(\"data\")\n",
    "    \n",
    "    os.remove('temp.zip')\n",
    "    os.remove('data/_about.txt')\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/' + get_data('http://www.manythings.org/anki/spa-eng.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/spa.txt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lines(line):\n",
    "    \n",
    "    from unicodedata import normalize\n",
    "    import string\n",
    "    \n",
    "    remove_punct_map = dict.fromkeys(map(ord, string.punctuation)) # thank you Reed!\n",
    "    \n",
    "    line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "    line = line.decode('UTF-8')\n",
    "    line = line.split()\n",
    "    line = [word.lower() for word in line]\n",
    "    line = [word.translate(remove_punct_map) for word in line]\n",
    "    line = [word for word in line if word.isalpha()]\n",
    "    \n",
    "    return(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(filepath):\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    file = open(filepath, mode='rt')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    \n",
    "    lines = text.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in  lines]\n",
    "    \n",
    "    all_cleaned = []\n",
    "    \n",
    "\n",
    "    \n",
    "    for pair in pairs:\n",
    "        cleaned_pair = []\n",
    "        for sentence in pair:\n",
    "            clean_sentence = clean_lines(sentence)\n",
    "            cleaned_pair.append(' '.join(clean_sentence))\n",
    "            \n",
    "        all_cleaned.append(cleaned_pair)\n",
    "    \n",
    "    no_duplicates = []\n",
    "    previous_eng = ''\n",
    "    previous_targ = ''\n",
    "    \n",
    "    for line in all_cleaned:\n",
    "        if (line[0] != previous_eng) and (line[1] != previous_targ) and (len(line[1]) != 1) and (len(line[0]) != 1) and (line[0] != '') and (line[1] != '') and (' tom' not in line[0]) and ('tom ' not in line[0]) and (' tom ' not in line[0]):\n",
    "            no_duplicates.append(line)\n",
    "            previous_eng = line[0]\n",
    "            previous_targ = line[1]\n",
    "          \n",
    "        \n",
    "    return np.array(no_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = clean_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['im hurt bad', 'estoy gravemente herido'],\n",
       "       ['try to focus', 'intenta centrarte'],\n",
       "       ['this is not true', 'esto no es verdad'],\n",
       "       ['i was not drunk', 'no estaba borracho'],\n",
       "       ['toms awake', 'tom esta despierto'],\n",
       "       ['come up here', 'sube aqui'],\n",
       "       ['now its my turn', 'ahora es mi turno'],\n",
       "       ['hes a bad liar', 'el es un mal mentiroso'],\n",
       "       ['tell the truth', 'deci la verdad'],\n",
       "       ['how unfortunate', 'que desafortunado']], dtype='<U275')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83627, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split in test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset, model_name):\n",
    "    \n",
    "    import numpy as np\n",
    "    from sklearn.externals import joblib\n",
    "    import os\n",
    "    \n",
    "    lines_number = 50000\n",
    "    new_set = dataset[:lines_number, :]\n",
    "    \n",
    "    np.random.shuffle(new_set)\n",
    "    train, test = new_set[:45000], new_set[45000:]\n",
    "    \n",
    "    directory = ('data/' + model_name)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "#     joblib.dump(new_set, (directory + '/dataset.pkl'))\n",
    "#     joblib.dump(train, (directory + '/train.pkl'))\n",
    "#     joblib.dump(test, (directory + '/test.pkl'))\n",
    "    \n",
    "    return new_set, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT FIELD!!!!!!!!!!!!!!\n",
    "# create a unique model name otherwise everything will be overwritten\n",
    "new_model_name = 'model_50K'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, train, test = split_data(clean_data, new_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/model_01081_32_128/test.pkl']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save datasets for evaluation\n",
    "joblib.dump(train, ('data/' + new_model_name + '/train.pkl'))\n",
    "joblib.dump(test, ('data/' + new_model_name + '/test.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9000"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['shes turning red', 'ella se sonroja'],\n",
       "       ['i liked it', 'me gusto'],\n",
       "       ['i was impressed', 'estaba impresionada'],\n",
       "       ['youve done it', 'lo has hecho'],\n",
       "       ['my father is in', 'mi padre esta en casa'],\n",
       "       ['i was about to go', 'estaba por irme'],\n",
       "       ['what a phony', 'que farsante'],\n",
       "       ['that helped', 'eso ayudo'],\n",
       "       ['thats not my job', 'ese no es mi trabajo'],\n",
       "       ['he has a point', 'en eso tiene razon']], dtype='<U275')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[500:510]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sonik/anaconda3/envs/pythondata/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a number of unique words in english dataset - 2962\n",
      "max sentence size english dataset - 5\n",
      "a number of unique words in target dataset - 4661\n",
      "max sentence size target dataset - 9\n"
     ]
    }
   ],
   "source": [
    "eng_tokenizer = Tokenizer()\n",
    "eng_tokenizer.fit_on_texts(dataset[:,0])\n",
    "eng_length = max(len(line.split()) for line in dataset[:,0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_counts) + 1\n",
    "targ_tokenizer = Tokenizer()\n",
    "targ_tokenizer.fit_on_texts(dataset[:,1])\n",
    "targ_length = max(len(line.split()) for line in dataset[:,1])\n",
    "targ_vocab_size = len(targ_tokenizer.word_counts) + 1\n",
    "print(f'a number of unique words in english dataset - {len(eng_tokenizer.word_counts)}')\n",
    "print(f'max sentence size english dataset - {eng_length}')\n",
    "print(f'a number of unique words in target dataset - {len(targ_tokenizer.word_counts)}')\n",
    "print(f'max sentence size target dataset - {targ_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/model_01081_32_128/targ_length.pkl']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save tokenizers and vocab sizes for further decoding\n",
    "joblib.dump(eng_tokenizer, ('data/' + new_model_name + '/eng_tokenizer.pkl'))\n",
    "joblib.dump(eng_length, ('data/' + new_model_name + '/eng_length.pkl'))\n",
    "joblib.dump(targ_tokenizer, ('data/' + new_model_name + '/targ_tokenizer.pkl'))\n",
    "joblib.dump(targ_length, ('data/' + new_model_name + '/targ_length.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = eng_tokenizer.texts_to_sequences(train[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 385, 128],\n",
       " [8, 83, 3, 129],\n",
       " [32, 4, 2860],\n",
       " [11, 4, 526],\n",
       " [6, 28, 296],\n",
       " [48, 98, 152],\n",
       " [11, 58, 1, 41],\n",
       " [22, 1046],\n",
       " [90, 4, 737],\n",
       " [32, 13, 250]]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = pad_sequences(trainX, maxlen=eng_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,  385,  128,    0,    0],\n",
       "       [   8,   83,    3,  129,    0],\n",
       "       [  32,    4, 2860,    0,    0],\n",
       "       ...,\n",
       "       [  13,  561, 1395,    0,    0],\n",
       "       [   1,   25,  203,    0,    0],\n",
       "       [   1,   27,    8, 2115,    0]], dtype=int32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/model_01081_32_128/trainX.pkl']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(trainX, ('data/' + new_model_name + '/trainX.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = targ_tokenizer.texts_to_sequences(train[:,1])\n",
    "trainY = pad_sequences(trainY, maxlen=targ_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 922,  103,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   2,  155,    6,  129,    0,    0,    0,    0,    0],\n",
       "       [  33,    5, 4469,    0,    0,    0,    0,    0,    0],\n",
       "       [  66,    1,    5,  431,    0,    0,    0,    0,    0],\n",
       "       [  19,   24,  358,    0,    0,    0,    0,    0,    0]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = to_categorical(trainY, num_classes=targ_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  59,  239,  249,    0,    0],\n",
       "       [   1,   41,  124,  368,    0],\n",
       "       [  52,    3,  192,    0,    0],\n",
       "       ...,\n",
       "       [ 108,   21,   13,  642,    0],\n",
       "       [  13, 1497,   14,  126,    0],\n",
       "       [2335,   53,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX = eng_tokenizer.texts_to_sequences(test[:,0])\n",
    "testX = pad_sequences(testX, maxlen=eng_length, padding='post')\n",
    "testX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/model_01081_32_128/testX.pkl']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(testX, ('data/' + new_model_name + '/testX.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY = targ_tokenizer.texts_to_sequences(test[:,1])\n",
    "testY = pad_sequences(testY, maxlen=targ_length, padding='post')\n",
    "testY = to_categorical(testY, num_classes=targ_vocab_size)\n",
    "testY[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(eng_vocab_size, 256, input_length=eng_length, mask_zero=True))\n",
    "model.add(LSTM(256))\n",
    "model.add(RepeatVector(targ_length))\n",
    "# model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(targ_vocab_size, activation='softmax')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 5, 256)            758528    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_3 (RepeatVecto (None, 9, 256)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 9, 512)            1050624   \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 9, 4662)           2391606   \n",
      "=================================================================\n",
      "Total params: 4,726,070\n",
      "Trainable params: 4,726,070\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'data/' + new_model_name + '/' + new_model_name + '.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(model_name, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      " - 47s - loss: 2.9965 - acc: 0.6593 - val_loss: 2.4787 - val_acc: 0.6586\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.47871, saving model to data/model_01081_32_128/model_01081_32_128.h5\n",
      "Epoch 2/30\n",
      " - 48s - loss: 2.3515 - acc: 0.6700 - val_loss: 2.4437 - val_acc: 0.6636\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.47871 to 2.44374, saving model to data/model_01081_32_128/model_01081_32_128.h5\n",
      "Epoch 3/30\n",
      " - 48s - loss: 2.2819 - acc: 0.6737 - val_loss: 2.4211 - val_acc: 0.6689\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.44374 to 2.42113, saving model to data/model_01081_32_128/model_01081_32_128.h5\n",
      "Epoch 4/30\n",
      " - 47s - loss: 2.1971 - acc: 0.6785 - val_loss: 2.3582 - val_acc: 0.6738\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.42113 to 2.35819, saving model to data/model_01081_32_128/model_01081_32_128.h5\n",
      "Epoch 5/30\n",
      " - 48s - loss: 2.0891 - acc: 0.6868 - val_loss: 2.2992 - val_acc: 0.6829\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.35819 to 2.29917, saving model to data/model_01081_32_128/model_01081_32_128.h5\n",
      "Epoch 6/30\n",
      " - 48s - loss: 1.9813 - acc: 0.6956 - val_loss: 2.2291 - val_acc: 0.6910\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.29917 to 2.22907, saving model to data/model_01081_32_128/model_01081_32_128.h5\n",
      "Epoch 7/30\n",
      " - 47s - loss: 1.8656 - acc: 0.7061 - val_loss: 2.1834 - val_acc: 0.6989\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.22907 to 2.18338, saving model to data/model_01081_32_128/model_01081_32_128.h5\n",
      "Epoch 8/30\n",
      " - 48s - loss: 1.7504 - acc: 0.7167 - val_loss: 2.1257 - val_acc: 0.7078\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.18338 to 2.12567, saving model to data/model_01081_32_128/model_01081_32_128.h5\n",
      "Epoch 9/30\n",
      " - 57s - loss: 1.6286 - acc: 0.7282 - val_loss: 2.0724 - val_acc: 0.7152\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.12567 to 2.07240, saving model to data/model_01081_32_128/model_01081_32_128.h5\n",
      "Epoch 10/30\n",
      " - 53s - loss: 1.5143 - acc: 0.7382 - val_loss: 2.0399 - val_acc: 0.7199\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.07240 to 2.03988, saving model to data/model_01081_32_128/model_01081_32_128.h5\n",
      "Epoch 11/30\n",
      " - 48s - loss: 1.4066 - acc: 0.7467 - val_loss: 2.0137 - val_acc: 0.7253\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.03988 to 2.01367, saving model to data/model_01081_32_128/model_01081_32_128.h5\n",
      "Epoch 12/30\n",
      " - 45s - loss: 1.3052 - acc: 0.7550 - val_loss: 1.9792 - val_acc: 0.7312\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.01367 to 1.97916, saving model to data/model_01081_32_128/model_01081_32_128.h5\n",
      "Epoch 13/30\n",
      " - 43s - loss: 1.2070 - acc: 0.7634 - val_loss: 1.9820 - val_acc: 0.7308\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.97916\n",
      "Epoch 14/30\n",
      " - 43s - loss: 1.1158 - acc: 0.7720 - val_loss: 1.9605 - val_acc: 0.7351\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.97916 to 1.96046, saving model to data/model_01081_32_128/model_01081_32_128.h5\n",
      "Epoch 15/30\n",
      " - 43s - loss: 1.0296 - acc: 0.7804 - val_loss: 1.9528 - val_acc: 0.7367\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.96046 to 1.95279, saving model to data/model_01081_32_128/model_01081_32_128.h5\n",
      "Epoch 16/30\n",
      " - 40s - loss: 0.9483 - acc: 0.7909 - val_loss: 1.9428 - val_acc: 0.7369\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.95279 to 1.94282, saving model to data/model_01081_32_128/model_01081_32_128.h5\n",
      "Epoch 17/30\n",
      " - 37s - loss: 0.8728 - acc: 0.8012 - val_loss: 1.9430 - val_acc: 0.7377\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.94282\n",
      "Epoch 18/30\n",
      " - 37s - loss: 0.8023 - acc: 0.8158 - val_loss: 1.9309 - val_acc: 0.7411\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.94282 to 1.93086, saving model to data/model_01081_32_128/model_01081_32_128.h5\n",
      "Epoch 19/30\n",
      " - 37s - loss: 0.7294 - acc: 0.8290 - val_loss: 1.9380 - val_acc: 0.7421\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.93086\n",
      "Epoch 20/30\n",
      " - 34s - loss: 0.6668 - acc: 0.8446 - val_loss: 1.9462 - val_acc: 0.7384\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.93086\n",
      "Epoch 21/30\n",
      " - 35s - loss: 0.6083 - acc: 0.8581 - val_loss: 1.9371 - val_acc: 0.7436\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.93086\n",
      "Epoch 22/30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-6b519411eed6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pythondata/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/pythondata/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pythondata/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pythondata/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pythondata/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
