{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sonik/anaconda3/envs/pythondata/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a model name to load\n",
    "model_name = 'model_01081_32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = joblib.load('data/dataset.pkl')\n",
    "train = joblib.load('data/' + model_name + '/train.pkl')\n",
    "test = joblib.load('data/' + model_name + '/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenizer = joblib.load('data/' + model_name + '/eng_tokenizer.pkl')\n",
    "eng_length = joblib.load('data/' + model_name + '/eng_length.pkl')\n",
    "eng_vocab_size = len(eng_tokenizer.word_counts) + 1\n",
    "targ_tokenizer = joblib.load('data/' + model_name + '/targ_tokenizer.pkl')\n",
    "targ_length = joblib.load('data/' + model_name + '/targ_length.pkl')\n",
    "targ_vocab_size = len(targ_tokenizer.word_counts) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = joblib.load('data/' + model_name + '/trainX.pkl')\n",
    "testX = joblib.load('data/' + model_name + '/testX.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenizer = Tokenizer()\n",
    "eng_tokenizer.fit_on_texts(dataset[:,0])\n",
    "eng_length = max(len(line.split()) for line in dataset[:,0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_counts) + 1\n",
    "targ_tokenizer = Tokenizer()\n",
    "targ_tokenizer.fit_on_texts(dataset[:,1])\n",
    "targ_length = max(len(line.split()) for line in dataset[:,1])\n",
    "targ_vocab_size = len(targ_tokenizer.word_counts) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  36, 1133,    0,    0,    0],\n",
       "       [   7,   19,   54,  126,    0],\n",
       "       [  18,  674,    2,    0,    0],\n",
       "       ...,\n",
       "       [   7,  714, 2831,    0,    0],\n",
       "       [   3,    5,  210,    2,    0],\n",
       "       [   1,  902,    5,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainX = eng_tokenizer.texts_to_sequences(train[:,0])\n",
    "# trainX = pad_sequences(trainX, maxlen=eng_length, padding='post')\n",
    "trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6, 544, 617,   0,   0],\n",
       "       [  8, 176,   3, 387,   0],\n",
       "       [  2,  20, 323,  82,   0],\n",
       "       ...,\n",
       "       [  1, 271,  69,   0,   0],\n",
       "       [  7,  19, 430,   0,   0],\n",
       "       [  7,  76, 307, 313,   0]], dtype=int32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testX = eng_tokenizer.texts_to_sequences(test[:,0])\n",
    "# testX = pad_sequences(testX, maxlen=eng_length, padding='post')\n",
    "testX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('data/' + model_name + '/' + model_name + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    " \n",
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)\n",
    " \n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "    actual, predicted = list(), list()\n",
    "    for i, source in enumerate(sources):\n",
    "        # translate encoded source text\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        translation = predict_sequence(model, targ_tokenizer, source)\n",
    "        raw_src, raw_target = raw_dataset[i]\n",
    "        if i < 10:\n",
    "            print(f'src: {raw_src}; target: {raw_target}; translation: {translation}')\n",
    "        actual.append([raw_target.split()])\n",
    "        predicted.append(translation.split())\n",
    "    # calculate BLEU score\n",
    "    print(f'BLEU-1: {corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))}')\n",
    "    print(f'BLEU-2: {corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))}')\n",
    "    print(f'BLEU-3: {corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0))}')\n",
    "    print(f'BLEU-4: {corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))}')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "src: thats helpful; target: eso es de ayuda; translation: eso es ayuda ayuda\n",
      "src: he was very tired; target: el estaba muy cansado; translation: el estaba muy cansado\n",
      "src: she trusted you; target: ella confiaba en ti; translation: ella confiaba en en\n",
      "src: im just curious; target: tan solo soy curioso; translation: solo estoy soy\n",
      "src: i also like cakes; target: tambien me gusta los pasteles; translation: me gusta el los\n",
      "src: see for yourself; target: miralo tu mismo; translation: miralo tu\n",
      "src: can i have some; target: me das un poco; translation: me un un mas\n",
      "src: thats my fault; target: eso es mi culpa; translation: es es culpa\n",
      "src: i didnt hear you; target: yo no te oi; translation: no no he oi\n",
      "src: i need my pills; target: necesito mis pastillas; translation: necesito mis pastillas\n",
      "BLEU-1: 0.7563666277049743\n",
      "BLEU-2: 0.6575261754621241\n",
      "BLEU-3: 0.5592416169792461\n",
      "BLEU-4: 0.33944535229828865\n"
     ]
    }
   ],
   "source": [
    "print('train')\n",
    "evaluate_model(model, targ_tokenizer, trainX, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "src: im being careful; target: estoy siendo cuidadoso; translation: estoy impresionada\n",
      "src: the room is dark; target: la habitacion es oscura; translation: el habitacion esta echada\n",
      "src: you can trust us; target: puedes confiar en nosotros; translation: puedes puedes con\n",
      "src: i was speechless; target: me quede sin palabras; translation: estaba avergonzado\n",
      "src: am i the only one; target: soy el unico; translation: estoy solo de\n",
      "src: he went back home; target: el volvio a casa; translation: el a a a\n",
      "src: have a nice trip; target: buen viaje; translation: se un un\n",
      "src: im immune; target: soy inmune; translation: estoy confiable\n",
      "src: i made fun of him; target: me burle de el; translation: le hice por\n",
      "src: do you feel bad; target: esta mal; translation: me siento mal\n",
      "BLEU-1: 0.38148339343680593\n",
      "BLEU-2: 0.24823427433392617\n",
      "BLEU-3: 0.17434706388794965\n",
      "BLEU-4: 0.07528697760240183\n"
     ]
    }
   ],
   "source": [
    "print('test')\n",
    "evaluate_model(model, targ_tokenizer, testX, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test')\n",
    "evaluate_model(model, targ_tokenizer, testX, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
